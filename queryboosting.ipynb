{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2Q9-w3DAwgG0"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "%pip install -qU \\\n",
        "    openai==0.27.7 \\\n",
        "    \"pinecone-client[grpc]\"==2.2.1 \\\n",
        "    datasets==2.12.0 \\\n",
        "    tqdm\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n",
        "!pip install --upgrade transformers\n",
        "!pip install --upgrade protobuf\n",
        "\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "from sentence_transformers import SentenceTransformer\n",
        "encoder = SentenceTransformer('paraphrase-MiniLM-L6-v2') # out of the box sentence embedder from hugging face"
      ],
      "metadata": {
        "id": "g3av8kktZY6d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Gv-kVj4lwgG2"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "key = ''\n",
        "openai.api_key = key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-UxCqZU8wsBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import tarfile\n",
        "\n",
        "# Specify the path to the gzipped file and the destination path for the uncompressed file\n",
        "tar_gz_file_path = '/content/drive/MyDrive/triviaqa-rc.tar.gz'\n",
        "extraction_directory = '/content/trivia-qc'  # You can change this path\n",
        "\n",
        "import os\n",
        "os.makedirs(extraction_directory, exist_ok=True)\n",
        "\n",
        "# Open and extract the .tar.gz file\n",
        "with tarfile.open(tar_gz_file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extraction_directory)\n",
        "\n",
        "# Verify that the file has been successfully uncompressed\n",
        "!ls /content\n"
      ],
      "metadata": {
        "id": "wqhhbSSWyTob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9LqmidvawgG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36949a20-ad7f-4b77-e555-0c4edc5aa0c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the current state-of-the art in question answering?\n",
            "{\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"finish_reason\": \"stop\",\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"content\": \"The current state-of-the-art in question answering involves the use of advanced natural language processing and machine learning techniques. This includes methods such as deep learning, transformers, and language models. One of the most notable advancements in recent years has been the development of pre-trained language models, such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which have achieved impressive results in question answering tasks.\\n\\nThese models are trained on large amounts of text data and can understand the context and meaning of words and sentences. They can be fine-tuned for specific question answering tasks and are capable of answering a wide range of questions across different domains.\\n\\nAnother notable advancement is the use of neural networks and attention mechanisms to improve the performance of question answering systems. Attention mechanisms allow the model to focus on important parts of the input and make more accurate predictions.\\n\\nAdditionally, advancements in knowledge graph representations and reasoning techniques have been applied to question answering, enabling systems to leverage structured knowledge bases to provide more accurate and coherent answers.\\n\\nOverall, the current state-of-the-art question answering systems have greatly improved their ability to understand and answer a wide range of questions accurately and efficiently. However, there are still challenges, such as handling ambiguous or complex queries, incorporating external knowledge, and understanding context-dependent information, which continue to be areas of active research.\",\n",
            "        \"role\": \"assistant\"\n",
            "      }\n",
            "    }\n",
            "  ],\n",
            "  \"created\": 1695783596,\n",
            "  \"id\": \"chatcmpl-83F4GWL4wxoHWD0SYzxcDasVO97jT\",\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"usage\": {\n",
            "    \"completion_tokens\": 270,\n",
            "    \"prompt_tokens\": 20,\n",
            "    \"total_tokens\": 290\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "question = 'What is the current state-of-the art in question answering?'\n",
        "prompt_start = '''\n",
        "{question}.'''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=[{\"role\": \"user\", \"content\": prompt_start.format(question=question)}],\n",
        "          stop=None\n",
        "      )\n",
        "\n",
        "print(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pTJK12wSwgG4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05a906e7-96af-4b5f-f617-1c83f692e8f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question answering (QA) has been a focus of research in the field of natural language processing (NLP) for several years. The current state-of-the-art in QA has seen significant advancements, mainly due to the development of deep learning models and the availability of large-scale datasets.\n",
            "\n",
            "One of the most notable advancements in QA is the introduction of transformer-based models, particularly the BERT (Bidirectional Encoder Representations from Transformers) model. BERT has revolutionized QA by pretraining a language model on massive amounts of text data, enabling it to understand the context and relationships between words. This has led to significant improvements in accuracy and performance.\n",
            "\n",
            "Another significant development is the use of transfer learning techniques. Instead of training models from scratch, researchers have leveraged pretrained models on large-scale language tasks like language modeling, masked language modeling, and next sentence prediction. These models are then fine-tuned on specific QA tasks, resulting in better generalization and performance.\n",
            "\n",
            "With the advent of large-scale QA datasets like SQuAD (Stanford Question Answering Dataset) and MS MARCO (Microsoft Machine Reading Comprehension), researchers now have access to high-quality training data for improving QA models. These datasets contain human-generated questions and answers, enabling models to learn from human expertise and achieve higher accuracy.\n",
            "\n",
            "Additionally, recent research has focused on utilizing external knowledge sources like Wikipedia or web documents to enhance QA systems. By incorporating external information, models can tackle questions that go beyond the scope of the dataset used for training.\n",
            "\n",
            "Overall, the current state-of-the-art in QA involves the use of transformer-based models, transfer learning techniques, large-scale datasets, and the incorporation of external knowledge sources. These advancements have significantly improved the accuracy and performance of QA systems, bringing us closer to more human-like question answering capabilities.\n"
          ]
        }
      ],
      "source": [
        "## make a call to the openai API asking chatGPT to produce context from a prompt that could be answered from that context\n",
        "\n",
        "# What prompt should we add in front of the querty?\n",
        "\n",
        "question = 'What is the current state-of-the art in question answering?'\n",
        "prompt_start = ''' Generate a background document to answer the given question in less than 200 words:\n",
        "{question}.'''\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "          model=\"gpt-3.5-turbo\",\n",
        "          messages=[{\"role\": \"user\", \"content\": prompt_start.format(question=question)}],\n",
        "          stop=None\n",
        "      )\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define a function for recursive directory traversal\n",
        "def collect_file_names(directory):\n",
        "    file_names = []  # Initialize an empty list to store file names\n",
        "\n",
        "    # Loop through all items (files and subdirectories) in the current directory\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)  # Get the full path of the item\n",
        "\n",
        "        # Check if the item is a file\n",
        "        if os.path.isfile(item_path):\n",
        "            file_names.append(item_path)  # Add the file path to the list\n",
        "        elif os.path.isdir(item_path):\n",
        "            # If the item is a directory, recursively collect file names from it\n",
        "            subdirectory_files = collect_file_names(item_path)\n",
        "            file_names.extend(subdirectory_files)  # Extend the list with file names from the subdirectory\n",
        "\n",
        "    return file_names"
      ],
      "metadata": {
        "id": "C_kMAKZW8V4W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XefVc9-awgG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19b97848-0357-4242-a99f-a7334e020e7e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start embedding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 149/149 [49:00<00:00, 19.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done embedding\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import openai\n",
        "import nltk\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "\n",
        "corpus_path = '/content/trivia-qc/evidence/wikipedia'\n",
        "\n",
        "vector_db = []\n",
        "files = collect_file_names('/content/trivia-qc/evidence/wikipedia')\n",
        "print('start embedding')\n",
        "\n",
        "\n",
        "# ## randomly sample from the wikipedia-test-dataset\n",
        "# with open('/content/trivia-qc/qa/wikipedia-train.json') as json_file:\n",
        "#     data = json.load(json_file)\n",
        "#     trivia_data = data['Data']\n",
        "\n",
        "# # for all trivia questions, go to the file that is named under entitytexts and embed the text in 300 word chunks\n",
        "# for json_obj in tqdm(trivia_data):\n",
        "#     filenames = [json_obj['EntityPages'][i]['Filename'] for i in range(len(json_obj['EntityPages']))]\n",
        "    # open file named filename under the evidence folder\n",
        "\n",
        "batch_size = 500\n",
        "file_batches = [files[i:i+batch_size] for i in range(0, len(files), batch_size)]\n",
        "\n",
        "\n",
        "for files in tqdm(file_batches):\n",
        "  batchchunks = []\n",
        "  for filename in files:\n",
        "    try:\n",
        "        with open(filename) as text_file:\n",
        "            # load text file\n",
        "            data = text_file.read()\n",
        "            words = nltk.word_tokenize(data)\n",
        "\n",
        "            chunks = []  # To store the 500-word chunks\n",
        "            current_chunk = []  # To store the current chunk\n",
        "\n",
        "            # Maximum number of words per chunk\n",
        "            max_words_per_chunk = 400\n",
        "\n",
        "            # Iterate through the words\n",
        "            for word in words:\n",
        "                current_chunk.append(word)\n",
        "\n",
        "                # Check if the current chunk has reached the maximum word count\n",
        "                if len(current_chunk) >= max_words_per_chunk:\n",
        "                    # Add the current chunk to the list of chunks\n",
        "                    chunks.append(current_chunk)\n",
        "                    # Start a new chunk\n",
        "                    current_chunk = []\n",
        "            # Add any remaining words as a final chunk\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "            # Combine the chunks into strings\n",
        "            chunked_texts = [' '.join(chunk) for chunk in chunks]\n",
        "            batchchunks += chunked_texts\n",
        "    except Exception as e:\n",
        "      print(e)\n",
        "      break\n",
        "\n",
        "\n",
        "  # embed each chunk\n",
        "  # try:\n",
        "    # embeddings = openai.Embedding.create(input=batchchunks, model=\"text-embedding-ada-002\")\n",
        "  embeddings = encoder.encode(batchchunks)\n",
        "  for j in range(len(embeddings)):\n",
        "      # create a json object with the embedding and the source\n",
        "      vector = {\n",
        "          'embedding': embeddings[j].tolist(),\n",
        "          'meta': {\n",
        "              'sources': filename,\n",
        "              'text': batchchunks[j],\n",
        "          }\n",
        "      }\n",
        "      # append the json object to the vector_db\n",
        "      vector_db.append(vector)\n",
        "  # except Exception as e:\n",
        "  #     print(e)\n",
        "  #     break\n",
        "\n",
        "print('done embedding')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(vector_db))\n",
        "import pickle\n",
        "\n",
        "# Specify the file path where you want to save the dictionary\n",
        "file_path = '/content/vector_db.json'  # You can change the file path as needed\n",
        "\n",
        "with open(file_path, 'w') as json_file:\n",
        "    json.dump(vector_db, json_file)\n",
        "\n",
        "# Verify that the dictionary has been saved\n",
        "print(f\"Dictionary saved to {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQvqcla4KUSD",
        "outputId": "d67beee0-249e-480e-c240-32b9f6859ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming vector_db is a list of dictionaries\n",
        "# Extract the embeddings from the list and stack them vertically to form a matrix\n",
        "embedding_matrix = np.vstack([vector['embedding'] for vector in vector_db])\n",
        "np.save('/content/embedding_matrix.npy', embedding_matrix)"
      ],
      "metadata": {
        "id": "wpH059YyVQ2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "# from timeout_decorator import timeout\n",
        "\n",
        "accuracyq = 0\n",
        "accuracyfc = 0\n",
        "total = len(trivia_data)\n",
        "\n",
        "# Function to perform operations on a question\n",
        "def process_question(question):\n",
        "    ## ask chatgpt the question using the openai api\n",
        "    prompt_start = ''' Generate a background document to answer the given question in less than 200 words:\n",
        "    {question}.'''\n",
        "    try:\n",
        "      response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt_start.format(question=question)}],\n",
        "            stop=None\n",
        "        )\n",
        "    except:\n",
        "      return None\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "print('starting query boosting')\n",
        "# Iterate through the list of train data jsons\n",
        "for json_obj in tqdm(trivia_data):\n",
        "    # Extract the question from the JSON object\n",
        "    question = json_obj[\"Question\"]\n",
        "    question_sources = [json_obj[\"EntityPages\"][i]['Filename'] for i in range(len(json_obj[\"EntityPages\"]))]\n",
        "\n",
        "\n",
        "    # Perform operations on the question\n",
        "    fake_context = process_question(question)\n",
        "\n",
        "    if fake_context is None:\n",
        "      continue\n",
        "\n",
        "    ## embed the fake context using the openai api embeddings endpoint\n",
        "\n",
        "    try:\n",
        "      embeddings = openai.Embedding.create(input=[fake_context, question], model=\"text-embedding-ada-002\")\n",
        "      fake_context_embedding = embeddings.data[0].embedding\n",
        "      question_embedding = embeddings.data[1].embedding\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "\n",
        "    # search our vector_db\n",
        "\n",
        "    bestq = None\n",
        "    bestfc = None\n",
        "\n",
        "    resfc = np.dot(embedding_matrix, fake_context_embedding)\n",
        "    max_indexfc = np.argmax(resfc)\n",
        "\n",
        "    resq = np.dot(embedding_matrix, question_embedding)\n",
        "    max_indexq = np.argmax(resq)\n",
        "\n",
        "    # Check if the processed question matches the question source\n",
        "\n",
        "    bestq = vector_db[max_indexq]\n",
        "    bestfc = vector_db[max_indexfc]\n",
        "\n",
        "\n",
        "    # Check if the processed question matches the question source\n",
        "    if bestq['meta']['sources'] in question_sources:\n",
        "        accuracyq += 1\n",
        "    if bestfc['meta']['sources'] in question_sources:\n",
        "        accuracyfc += 1\n",
        "\n",
        "print('ACCURACY BASE: ', accuracyq/total)\n",
        "print('ACCURACY BOOSTED: ', accuracyfc/total)\n",
        "\n"
      ],
      "metadata": {
        "id": "wywQ6g2v3lkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DO06eVRsPiFk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}